{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Necessary Packages**"
      ],
      "metadata": {
        "id": "Y9ihdgNn0-c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries with specified versions and suppress progress bar output for cleaner installation\n",
        "!pip -qqq install pip --progress-bar off\n",
        "!pip -qqq install langchain-groq==0.1.3 --progress-bar off\n",
        "!pip -qqq install langchain==0.1.17 --progress-bar off\n",
        "!pip -qqq install llama-parse==0.1.3 --progress-bar off\n",
        "!pip -qqq install qdrant-client==1.9.1 --progress-bar off\n",
        "!pip -qqq install \"unstructured[md]\"==0.13.6 --progress-bar off\n",
        "!pip -qqq install fastembed==0.2.7 --progress-bar off\n",
        "!pip -qqq install flashrank==0.2.4 --progress-bar off\n",
        "!pip install gradio==3.38.0\n"
      ],
      "metadata": {
        "id": "LEUUTy9f04Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries and Set Up API Key**"
      ],
      "metadata": {
        "id": "5Nd_PfvD1Dw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for document processing, retrieval, and Q&A system\n",
        "import os\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Import components from LangChain and other packages for document processing\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Qdrant\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from llama_parse import LlamaParse\n",
        "import gradio as gr\n",
        "\n",
        "# Set up environment variable for API key\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n"
      ],
      "metadata": {
        "id": "eJm3xB2N1Hck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Helper Function for Output Formatting**"
      ],
      "metadata": {
        "id": "g_CEfmhG1OvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to format and print responses with word-wrapping for readability\n",
        "def print_response(response):\n",
        "    response_txt = response[\"result\"]\n",
        "    for chunk in response_txt.split(\"\\n\"):\n",
        "        if not chunk:\n",
        "            print()\n",
        "            continue\n",
        "        print(\"\\n\".join(textwrap.wrap(chunk, 100, break_long_words=False)))\n"
      ],
      "metadata": {
        "id": "g_F_NLq11LfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Upload PDF File**"
      ],
      "metadata": {
        "id": "XFnsCmqf1S3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to store uploaded files\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Use Google Colab's file upload feature to upload a PDF file\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    pdf_path = Path(f\"{filename}\")\n"
      ],
      "metadata": {
        "id": "WTfMcZmj1TBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up Parsing Instructions and Parse PDF Document**"
      ],
      "metadata": {
        "id": "Ui82LHop1ci6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parsing instructions for extracting content from the PDF in a structured way\n",
        "instruction = \"\"\"Use the following pieces of context to provide a concise answer\n",
        "to the question at the end but use at least summarize with 250 words with detailed\n",
        "explanations. It contains many tables. Try to be precise while answering the questions\"\"\"\n",
        "\n",
        "# Initialize the LlamaParse API to parse the uploaded PDF document\n",
        "parser = LlamaParse(\n",
        "    api_key=userdata.get(\"LLAMA_PARSE\"),\n",
        "    result_type=\"markdown\",\n",
        "    parsing_instruction=instruction,\n",
        "    max_timeout=5000,\n",
        ")\n",
        "\n",
        "# Parse the PDF document asynchronously\n",
        "llama_parse_documents = await parser.aload_data(str(pdf_path))\n",
        "parsed_doc = llama_parse_documents[0]\n"
      ],
      "metadata": {
        "id": "0PkJK8No1gWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save and Load the Parsed Document**"
      ],
      "metadata": {
        "id": "KV8MQ8dd1moq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save parsed content to a markdown file in the 'data' directory\n",
        "document_path = Path(\"data/parsed_document.md\")\n",
        "with document_path.open(\"a\") as f:\n",
        "    f.write(parsed_doc.text)\n",
        "\n",
        "# Load the saved markdown file as a document for further processing\n",
        "loader = UnstructuredMarkdownLoader(document_path)\n",
        "loaded_documents = loader.load()\n"
      ],
      "metadata": {
        "id": "RzlgcW4o1lps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Documents into Chunks**"
      ],
      "metadata": {
        "id": "OqsOJV851tkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the loaded documents into chunks for processing with LangChain, using specified chunk size and overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=128)\n",
        "docs = text_splitter.split_documents(loaded_documents)\n"
      ],
      "metadata": {
        "id": "ZdGSVIzY1sox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Embeddings and Vector**"
      ],
      "metadata": {
        "id": "DhagDsDQ1_t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the embeddings model and vector store for document retrieval\n",
        "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "qdrant = Qdrant.from_documents(\n",
        "    docs,\n",
        "    embeddings,\n",
        "    path=\"./database\",\n",
        "    collection_name=\"document_embeddings\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "X-6E5q9S2D3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up Retriever and Compressor**"
      ],
      "metadata": {
        "id": "MIhUZP_U2E41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up retriever to retrieve the top 5 most relevant document chunks based on search queries\n",
        "retriever = qdrant.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# Configure a compressor model to re-rank retrieved documents for more contextually relevant results\n",
        "compressor = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n"
      ],
      "metadata": {
        "id": "u2xXPV4b2IBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Language Model and Prompt Template**"
      ],
      "metadata": {
        "id": "7J_0x4kz2KJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the language model for question answering with specified parameters\n",
        "llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\n",
        "\n",
        "# Define the template for constructing the QA prompt\n",
        "prompt_template = \"\"\"\n",
        "Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Answer the question and provide additional helpful information,\n",
        "based on the pieces of information, if applicable. Be succinct.\n",
        "\n",
        "Responses should be properly formatted to be easily read.\n",
        "\"\"\"\n",
        "\n",
        "# Create a prompt instance with specified template and variables\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "q82igx1l2OuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up QA Chain**"
      ],
      "metadata": {
        "id": "NsAe_kbs2P-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure a QA chain with the language model, retriever, and prompt template for answering user queries\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=compression_retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt, \"verbose\": False},\n",
        ")\n"
      ],
      "metadata": {
        "id": "FNJf2qx_2VUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Gradio Interface for User Interaction**"
      ],
      "metadata": {
        "id": "zwyUNQX62WAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to handle user questions and return the answer from the QA chain\n",
        "def answer_question(question):\n",
        "    response = qa.invoke(question)\n",
        "    return response[\"result\"]\n",
        "\n",
        "# Set up a Gradio interface for asking questions and displaying answers\n",
        "demo = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(label=\"Question\"),\n",
        "    outputs=gr.Textbox(label=\"Answer\"),\n",
        ")\n",
        "\n",
        "# Launch the Gradio app to allow user interaction with the QA system\n",
        "demo.launch(debug=True, share=True)\n"
      ],
      "metadata": {
        "id": "uCFbqB1L2Z66"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}